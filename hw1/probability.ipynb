{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we looked at the problem of linear regression, and particularly about how some funny things can happen when we give a model too much freedom.  In particular, we saw that as models became more versatile and complex (perhaps by increasing the number of available degrees of freedom), they started to fit the data noise, rather than the underlying function.  We saw that one way to control this was to limit the degrees of freedom of a model (not always possible or straightforward), and the other was to explicitly add regularization.  But even then, this doesn't really give us alot of tangible insight into whether or not we're fitting the model or the noise, and because of this, we also don't have any understanding of how much we can trust the parameters that we find.  We can see this with a demonstration.  First, suppose there's some physical process that takes an input $x$, and outputs $y$ deterministically.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.figsize'] = [12,8]\n",
    "np.random.seed(42)\n",
    "\n",
    "def y(x):\n",
    "    return -1 + x + x**2 + 0*x**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have access to the generating function $y(x) = -1 + x + x^2$, but let's pretend that we don't.  Now, let's imagine that we want to infer the model $y$ by measuring the output $\\hat{y}=y(x) + \\epsilon$ at some discrete points $x\\in[0,1]$, where the measurement is subject to some random noise $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(11)\n",
    "x.sort()\n",
    "epsilon = 1e-1\n",
    "yhat = y(x) + epsilon*np.random.randn(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do the normal thing and fit the data to a polynomial of order, say, 3, then plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "X = np.vander(x,degree+1,increasing=True)\n",
    "w_0 = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,yhat))\n",
    "plt.plot(x,yhat,'k.')\n",
    "plt.plot(x,np.dot(X,w_0))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we take our measurements again, and do the same procedure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = y(x) + epsilon*np.random.randn(11)\n",
    "w_1 = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,yhat))\n",
    "plt.plot(x,yhat,'k.')\n",
    "plt.plot(x,np.dot(X,w_0))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fits look similar.  But how about the parameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_0)\n",
    "plt.plot(w_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, this is somewhat troubling.  Even though we know for certain that the model that is generating the data is the same, we're getting different parameter values each time.  This is especially troubling if our model parameters have a real-world meaning that we need to use.  \n",
    "\n",
    "What if we were to run the same experiment, say 10000 times though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = []\n",
    "N_experiments = 10000\n",
    "for i in range(N_experiments):\n",
    "    yhat = y(x) + epsilon*np.random.randn(11)\n",
    "    w_list.append(np.linalg.solve(np.dot(X.T,X),np.dot(X.T,yhat)))\n",
    "w_array = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(nrows=1,ncols=4,sharey=True)\n",
    "axs[0].hist(w_array[:,0],20,normed=True)\n",
    "axs[1].hist(w_array[:,1],20,normed=True)\n",
    "axs[2].hist(w_array[:,2],20,normed=True)\n",
    "axs[3].hist(w_array[:,3],20,normed=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_array.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can run our experiments an infinite number of times, the mean of all the different solutions for $w$ gets quite close to the correct value!  It even detected that we didn't need to include the cubic term.  Additionally, maybe the width of these histograms can give us a sense of how much trust we should be putting in a given parameter.  Doing it this way is clearly much better than dealing with all this silly regularization, right? Problem solved, see you next semester. \n",
    "\n",
    "But we have a bit of a problem.  We only have a certain amount of data.  But we still want to be able to access the kind of information that we have available to us from the above analysis.  The way to do this is, of course, to move to a way of thinking about these problems that sees data and parameters (and models even) as distributions of possible values (as histograms), rather than as single points, which allows us to fully embrace uncertainty.  But to get to this framework, we really need to understand probability first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0\n",
    "beta = 1./epsilon**2\n",
    "X = np.vander(x,degree+1,increasing=True)\n",
    "Sigma = np.linalg.inv(beta*np.dot(X.T,X) + alpha*np.eye(degree+1))\n",
    "w = np.dot(Sigma,np.dot(beta*X.T,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,yhat,'k.')\n",
    "plt.plot(x,np.dot(X,w))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(nrows=1,ncols=4,sharey=True)\n",
    "axs[0].hist(w_array[:,0],20,normed=True)\n",
    "axs[1].hist(w_array[:,1],20,normed=True)\n",
    "axs[2].hist(w_array[:,2],20,normed=True)\n",
    "axs[3].hist(w_array[:,3],20,normed=True)\n",
    "w0s = np.linspace(w_array[:,0].min(),w_array[:,0].max(),101)\n",
    "w1s = np.linspace(w_array[:,1].min(),w_array[:,1].max(),101)\n",
    "w2s = np.linspace(w_array[:,2].min(),w_array[:,2].max(),101)\n",
    "w3s = np.linspace(w_array[:,3].min(),w_array[:,3].max(),101)\n",
    "axs[0].plot(w0s,1./np.sqrt(2*np.pi*Sigma[0,0])*np.exp(-0.5*(w0s - w[0])**2/Sigma[0,0]))\n",
    "axs[1].plot(w1s,1./np.sqrt(2*np.pi*Sigma[1,1])*np.exp(-0.5*(w1s - w[1])**2/Sigma[1,1]))\n",
    "axs[2].plot(w2s,1./np.sqrt(2*np.pi*Sigma[2,2])*np.exp(-0.5*(w2s - w[2])**2/Sigma[2,2]))\n",
    "axs[3].plot(w3s,1./np.sqrt(2*np.pi*Sigma[3,3])*np.exp(-0.5*(w3s - w[3])**2/Sigma[3,3]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "xhat = np.linspace(0,1,101)\n",
    "Xhat = np.vander(xhat,degree+1,increasing=True)\n",
    "\n",
    "for i in range(2000):\n",
    "    w_rand = multivariate_normal(w,Sigma).rvs()\n",
    "    plt.plot(xhat,np.dot(Xhat,w_rand),'r-',alpha=0.005)\n",
    "plt.plot(x,yhat,'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
