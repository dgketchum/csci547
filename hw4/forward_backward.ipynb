{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code the \"occasionally dishonest casino\" model.  First, let's define transition probabilities and emission probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# 0-> fair die, 1-> loaded die \n",
    "states_possible = [0,1]\n",
    "\n",
    "# Possible outputs from the die\n",
    "observations_possible = [0,1,2,3,4,5]\n",
    "\n",
    "# Transition matrix\n",
    "A = np.array([[0.99,0.01],[0.03,0.97]])\n",
    "\n",
    "# Emission probabilities as a function of state\n",
    "E = np.array([[1./6.]*6,\n",
    "              [0.1,0.1,0.1,0.1,0.1,0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate 1000 observations given these statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emit(state):\n",
    "    roll = np.random.choice(observations_possible,p=E[state])\n",
    "    return roll\n",
    "\n",
    "def transition(state):\n",
    "    return np.random.choice(states_possible,p=A[state])\n",
    "\n",
    "states = [0]\n",
    "observations = [emit(states[0])]\n",
    "\n",
    "m = 1000\n",
    "\n",
    "for t in range(m-1):\n",
    "    states.append(transition(states[-1]))\n",
    "    observations.append(emit(states[-1]))\n",
    "    \n",
    "states = np.array(states)\n",
    "observations = np.array(observations)\n",
    "\n",
    "plt.plot(states,'r-')\n",
    "plt.plot(observations,'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know the transition matrix, let's imagine that our initial state probability is given by the stable distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,v = np.linalg.eig(A.T)\n",
    "p_steady = (v[:,0].real)/(v[:,0].real.sum())\n",
    "print (p_steady)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(x,z):\n",
    "    return E[z,x]\n",
    "\n",
    "def prediction(z):\n",
    "    return np.dot(z,A)\n",
    "\n",
    "def prior(z):\n",
    "    return p_steady[z]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute our initial state probability using Bayes' rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = observations[0]\n",
    "\n",
    "P_0 = likelihood(x_0,states_possible)*prior(states_possible)\n",
    "P_0/=P_0.sum()\n",
    "print (P_0)\n",
    "\n",
    "likelihoods = np.array([likelihood(o,states_possible) for o in observations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.zeros((m,2))\n",
    "alpha[0] = P_0\n",
    "for t in range(1,1000):\n",
    "    P_previous = alpha[t-1]\n",
    "    predictions = prediction(P_previous)   \n",
    "    numerator = likelihoods[t]*predictions\n",
    "    alpha[t] = numerator/numerator.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(states,'r-')\n",
    "plt.plot(observations,'k.')\n",
    "plt.plot(alpha[:,1],'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty good!  If we kept this tally going as we played the game, we could choose to change our bet when the loaded die was in use.  Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(states[:300],'r-')\n",
    "plt.plot(alpha[:300,1],'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the lag.  This is because our information flow is one-sided, purely from left to right, however this is unavoidable if we wish to operate in an on-line capacity.  However, let's imagine a different situation in which we were the state gaming board, and we wanted to tell when a dealer was cheating by examining security camera footage.  This is a slightly different problem because now we can condition on future events as well as past ones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = m-1\n",
    "beta = np.zeros((m,2))\n",
    "\n",
    "# Base case: always proportional to one\n",
    "beta[T] = 1.0\n",
    "beta[T]/=beta[T].sum()\n",
    "for t in range(T-1,-1,-1):\n",
    "    P_next = beta[t+1]\n",
    "    numerator = np.dot(A,P_next*likelihoods[t+1])\n",
    "    beta[t] = numerator/numerator.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can multiply the forward probabilities by the reverse probabilities and normalize to get the total probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_probs = alpha*beta\n",
    "total_probs = total_probs/total_probs.sum(axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(observation_list,'k.')\n",
    "plt.plot(total_probs[:300,1],'b-',lw=5.0)\n",
    "plt.plot(alpha[:300,1],'g-',lw=5.0)\n",
    "plt.plot(states[:300],'r-',lw=5.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we turn to the case in which we do not know the model *a priori*, which is to say that we do not know the state transition matrix $A$.  Perhapse we also don't know the emission probabilities.  How do we find these things?  \n",
    "In the case where we have observations of both the states and the observations, then reconstruction is very easy and proceeds as in the case of the non-hidden Markov model: for the transition matrix, we simply count the transitions from one state to the other, then normalize them afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_approx = np.zeros((2,2))\n",
    "prior_prob = np.array([np.sum(states==s) for s in states_possible])\n",
    "prior_prob = prior_prob/float(prior_prob.sum())\n",
    "\n",
    "for t in range(m-1):\n",
    "    i = states[t]\n",
    "    j = states[t+1]\n",
    "    A_approx[i,j] += 1\n",
    "    \n",
    "A_approx = A_approx/A_approx.sum(axis=1)[:,np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good recovery of the correct matrix.  Now we turn to recovery of the emission probability model.  This is also straightforward: we simply compute the frequency of each data point, given a particular state and place these in a table:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_approx = np.zeros((2,6))\n",
    "for z,x in zip(states,observations):\n",
    "    E_approx[z,x] += 1\n",
    "    \n",
    "E_approx = E_approx/E_approx.sum(axis=1)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (E_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is, of course, very similar to the correct emission probabilities.\n",
    "\n",
    "With these matrices in hand, it is easy to run the the forward-backward algorithm and infer the state probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(x,z):\n",
    "    return E_approx[z,x]\n",
    "\n",
    "def prediction(z):\n",
    "    return np.dot(z,A_approx)\n",
    "\n",
    "def prior(z):\n",
    "    return prior_prob[z]\n",
    "\n",
    "x_0 = observations[0]\n",
    "\n",
    "P_0 = likelihood(x_0,states_possible)*prior(states_possible)\n",
    "P_0/=P_0.sum()\n",
    "\n",
    "likelihoods = np.array([likelihood(o,states_possible) for o in observations])\n",
    "\n",
    "alpha = np.zeros((m,2))\n",
    "alpha[0] = P_0\n",
    "for t in range(1,1000):\n",
    "    P_previous = alpha[t-1]\n",
    "    predictions = prediction(P_previous)   \n",
    "    numerator = likelihoods[t]*predictions\n",
    "    alpha[t] = numerator/numerator.sum()\n",
    "    \n",
    "T = m-1\n",
    "beta = np.zeros((m,2))\n",
    "\n",
    "# Base case: always proportional to one\n",
    "beta[T] = 1.0\n",
    "beta[T]/=beta[T].sum()\n",
    "for t in range(T-1,-1,-1):\n",
    "    P_next = beta[t+1]\n",
    "    numerator = np.dot(A,P_next*likelihoods[t+1])\n",
    "    beta[t] = numerator/numerator.sum()\n",
    "\n",
    "total_probs = alpha*beta\n",
    "total_probs = total_probs/total_probs.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "plt.plot(total_probs[:300,1],'b-')\n",
    "plt.plot(states[:300],'r-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, we do a fairly good job.  Now, however, let's consider the more difficult case in which we only have access to the observations (the dice rolls), but not the states.  Thus we need to learn both the transition matrix and emission probabilities *only* with the rolls.  \n",
    "\n",
    "This is not dissimilar to a mixture model, with the states as the unobserved latent variables.  As in the case of the mixture model, we utilize the *expectation-maximization* algorithm.  Recall that for EM, we alternate between computing the probability of the states given the current transition matrix and emission probabilities.  We then compute the transition matrix and emission probabilities given the current states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = observations[0]\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Let's make a very vague guess regarding the transition matrix and emission probs.\n",
    "#A_0 = np.random.rand(2,2)\n",
    "#A_0/=A_0.sum(axis=1)[:,np.newaxis]\n",
    "A_0 = np.array([[0.7,0.3],[0.3,0.7]])\n",
    "\n",
    "E_0 = np.random.rand(2,6)\n",
    "E_0/=E_0.sum(axis=1)[:,np.newaxis]\n",
    "#E_0 = np.array([[1./6.]*6,[1./6.]*6])\n",
    "A = A_0\n",
    "E = E_0\n",
    "\n",
    "ces = []\n",
    "\n",
    "# Loop over the number of EM iterations:\n",
    "for f in range(150):\n",
    "    \n",
    "    # Compute the prior given the current estimate of A\n",
    "    w,v = np.linalg.eig(A.T)\n",
    "    unity_index = np.argmax(w)\n",
    "    p_steady = (v[:,unity_index].real)/(v[:,unity_index].real.sum())\n",
    "\n",
    "    # define the likelihood model.\n",
    "    def likelihood(x,z):\n",
    "        return E[z,x]\n",
    "\n",
    "    # define a prediction\n",
    "    def prediction(z):\n",
    "        return np.dot(z,A)\n",
    "\n",
    "    # define the prior\n",
    "    def prior(z):\n",
    "        return p_steady[z]\n",
    "\n",
    "    # Expectation step: Use the forward-backward algorithm to compute the probability distribution of \n",
    "    # states, as well as the joint distribution of adjacent states P(z_{t+1},z_t|x_t)\n",
    "\n",
    "    P_0 = likelihood(x_0,states_possible)*prior(states_possible)\n",
    "    P_0/=P_0.sum()\n",
    "\n",
    "    likelihoods = np.array([likelihood(o,states_possible) for o in observations])\n",
    "\n",
    "    alpha = np.zeros((m,2))\n",
    "    alpha[0] = P_0\n",
    "    for t in range(1,1000):\n",
    "        P_previous = alpha[t-1]\n",
    "        predictions = np.dot(P_previous,A)  \n",
    "        numerator = likelihoods[t]*predictions\n",
    "        alpha[t] = numerator/numerator.sum()\n",
    "    \n",
    "    T = m-1\n",
    "    beta = np.zeros((m,2))\n",
    "\n",
    "    # Base case: always proportional to one\n",
    "    beta[T] = 1.0\n",
    "    beta[T]/=beta[T].sum()\n",
    "    for t in range(T-1,-1,-1):\n",
    "        P_next = beta[t+1]\n",
    "        numerator = np.dot(A,P_next*likelihoods[t+1])\n",
    "        beta[t] = numerator/numerator.sum()\n",
    "\n",
    "    gamma = alpha*beta\n",
    "    gamma = gamma/gamma.sum(axis=1)[:,np.newaxis] \n",
    "    \n",
    "    sigma = np.zeros((m,2,2))\n",
    "    for t in range(m):\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                try:    \n",
    "                    sigma[t,i,j] = alpha[t,i]*A[i,j]*beta[t+1,j]*likelihoods[t+1,j]\n",
    "                except IndexError:\n",
    "                    sigma[t,i,j] = alpha[t,i]*A[i,j]\n",
    "                 \n",
    "        sigma[t,:,:] /= sigma[t,:,:].sum()\n",
    "\n",
    "    # Maximization step: Use the estimate of states to compute the maximum likelihood estimators for the \n",
    "    # transition matrix and emission probabilities.\n",
    "    A = sigma[:-1].sum(axis=0)/gamma[:-1].sum(axis=0)[:,np.newaxis]\n",
    "    E = np.array([gamma[observations==k].sum(axis=0)/gamma.sum(axis=0) for k in range(6)]).T\n",
    "    \n",
    "    cross_entropy = -np.sum(states*np.log(gamma[:,1]) + (1-states)*np.log(1-gamma[:,1]))\n",
    "    ces.append(cross_entropy)\n",
    "    #print (A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(gamma[:,1],'g-')\n",
    "plt.plot(observations,'k.')\n",
    "plt.plot(states,'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ces)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
